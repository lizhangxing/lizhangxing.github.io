{"meta":{"title":"pusheng","subtitle":"","description":"","author":"普生","url":"http://localhost:4000","root":"/"},"pages":[],"posts":[{"title":"学习率调整策略","slug":"xuexilv","date":"2023-08-16T02:20:53.000Z","updated":"2023-08-26T11:40:04.719Z","comments":true,"path":"post/58585.html","link":"","permalink":"http://localhost:4000/post/58585.html","excerpt":"","text":"学习率调整策略——lr_scheduler学习率是深度学习训练中至关重要的参数，很多时候一个合适的学习率才能发挥出模型的较大潜力。所以学习率调整策略同样至关重要，这篇博客介绍一下Pytorch中常见的学习率调整方法。 1.steplr这是最简单常用的学习率调整方法，每过step_size轮，将此前的学习率乘以gamma。 1scheduler=lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) 2. MultiStepLRMultiStepLR同样也是一个非常常见的学习率调整策略，它会在每个milestone时，将此前学习率乘以gamma。 1scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.5) 3. ExponentialLRExponentialLR是指数型下降的学习率调节器，每一轮会将学习率乘以gamma，所以这里千万注意gamma不要设置的太小，不然几轮之后学习率就会降到0。 1scheduler=lr_scheduler.ExponentialLR(optimizer, gamma=0.9) 4. LinearLRLinearLR是线性学习率，给定起始factor和最终的factor，LinearLR会在中间阶段做线性插值，比如学习率为0.1，起始factor为1，最终的factor为0.1，那么第0次迭代，学习率将为0.1，最终轮学习率为0.01。下面设置的总轮数total_iters为80,所以超过80时，学习率恒为0.01。 1scheduler=lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.1,total_iters=80) 5. OneCycleLROneCycleLR顾名思义就像是CyclicLR的一周期版本，它也有多个参数，max_lr就是最大学习率，pct_start是学习率上升部分所占比例，一开始的学习率为max_lr&#x2F;div_factor,最终的学习率为max_lr&#x2F;final_div_factor，总的迭代次数为total_steps。 1scheduler=lr_scheduler.OneCycleLR(optimizer,max_lr=0.1,pct_start=0.5,total_steps=120,div_factor=10,final_div_factor=10) 6. CosineAnnealingLRCosineAnnealingLR是余弦退火学习率，T_max是周期的一半，最大学习率在optimizer中指定，最小学习率为eta_min。这里同样能够帮助逃离鞍点。值得注意的是最大学习率不宜太大，否则loss可能出现和学习率相似周期的上下剧烈波动。 1scheduler=lr_scheduler.CosineAnnealingLR(optimizer,T_max=20,eta_min=0.05)","categories":[],"tags":[{"name":"提点","slug":"提点","permalink":"http://localhost:4000/tags/%E6%8F%90%E7%82%B9/"}]},{"title":"卷积计算量","slug":"卷积计算量的计算","date":"2023-08-14T13:48:55.000Z","updated":"2023-08-14T13:53:11.021Z","comments":true,"path":"post/24840.html","link":"","permalink":"http://localhost:4000/post/24840.html","excerpt":"","text":"卷积计算量的计算普通卷积当计算一个神经网络模型的 FLOPs 时，让我们以一个简化的卷积神经网络（CNN）为例。假设我们有一个包含两个卷积层和一个全连接层的模型。 假设模型的结构如下： 卷积层1：输入特征图大小为 32x32，卷积核大小为 3x3，输出通道数为 64。 池化层：最大池化，池化核大小为 2x2。 卷积层2：输入特征图大小为 16x16，卷积核大小为 3x3，输出通道数为 128。 全连接层：输入特征数为 128，输出特征数为 10。 我们将使用以下计算公式来估计 FLOPs： 对于卷积层，FLOPs &#x3D; 输入特征图大小 × 卷积核大小 × 输出通道数。 对于全连接层，FLOPs &#x3D; 输入特征数 × 输出特征数。 我们假设浮点运算包括乘法和加法。 卷积层1的 FLOPs： FLOPs &#x3D; 32x32x3x3x64 &#x3D; 1,179,648 池化层不涉及大量浮点运算，可以忽略。 卷积层2的 FLOPs： FLOPs &#x3D; 16x16x3x3x128 &#x3D; 1,179,648 全连接层的 FLOPs： FLOPs &#x3D; 128x10 &#x3D; 1,280 总的 FLOPs： 1,179,648 + 1,179,648 + 1,280 &#x3D; 2,360,576 所以，这个简化的模型的总 FLOPs 为约 2.36 百万次浮点运算。请注意，这只是一个简化的示例，实际的计算可能会更复杂，因为它还需要考虑其他层、操作和参数的影响。 深度可分离卷积深度可分离卷积是一种常用于轻量级神经网络中的卷积操作，它将标准卷积拆分为深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）。计算深度可分离卷积的 FLOPs 需要考虑这两个阶段的计算量。 以下是计算深度可分离卷积 FLOPs 的一般步骤： 计算深度卷积的 FLOPs： 深度卷积的计算量与输入通道数、卷积核大小、输出通道数和特征图大小有关。 对于输入通道数为 Cin，卷积核大小为 K，输出通道数为 Cout，特征图大小为 HxW（高度x宽度），深度卷积的 FLOPs 可以计算为 FLOPs_depthwise = Cin x K x K x H x W x Cout。 计算逐点卷积的 FLOPs： 逐点卷积的计算量与输出通道数、特征图大小有关。 对于输出通道数为 Cout，特征图大小为 HxW，逐点卷积的 FLOPs 可以计算为 FLOPs_pointwise = H x W x Cout。 总的 FLOPs： 深度可分离卷积的总 FLOPs 可以计算为 FLOPs_total = FLOPs_depthwise + FLOPs_pointwise。 这个计算过程可以帮助您估计深度可分离卷积的计算量。请注意，实际计算可能会因为批次大小、步幅等因素而有所不同，而且在深度学习框架中，使用内置的工具来计算 FLOPs 会更准确和方便。一些框架（如TensorFlow、PyTorch等）提供了用于估计模型计算量的功能。如果您想获得更精确的计算结果，建议使用这些工具来计算深度可分离卷积的 FLOPs。 组卷积组卷积（Group Convolution）是一种卷积操作的变体，它将输入特征图和卷积核分成若干组进行卷积操作。这种操作在一些神经网络中被用于提升计算效率和减少参数数量。组卷积主要应用于卷积神经网络（CNNs）中，常见于一些轻量级网络和深度可分离卷积中。 在标准的卷积操作中，每个输入通道与卷积核的所有通道进行卷积，这可能导致大量的参数和计算量。组卷积通过将输入通道和卷积核通道分成多个组，每个组内的通道进行独立的卷积操作，然后将各组的卷积结果叠加。这可以有效减少参数和计算量，特别是在需要保持较低计算量的场景中。 计算组卷积的 FLOPs（浮点运算次数）涉及以下几个步骤： 计算每个组内的卷积 FLOPs：对于每个组内的卷积，需要计算输入通道数、输出通道数、卷积核大小和特征图大小。对于输入通道数为 Cin，输出通道数为 Cout，卷积核大小为 K，特征图大小为 HxW，每个组内的卷积 FLOPs 可以计算为 FLOPs_group = Cin_group x K x K x H x W x Cout_group。 计算总的 FLOPs：将各个组内的卷积 FLOPs 相加，得到组卷积的总 FLOPs。 需要注意的是，组卷积可能会在一些网络架构中对不同的通道进行分组，以减少参数量和计算量。具体的分组策略可以根据网络结构和任务需求进行调整。 组卷积是一种可以用于优化卷积神经网络的技术，适用于需要减少参数和计算量的情况。在使用时，可以结合深度学习框架提供的工具来计算具体的 FLOPs，以便更准确地了解组卷积对模型计算量的影响。v s","categories":[],"tags":[]},{"title":"脑PET图像分析和疾病预测挑战赛","slug":"nao","date":"2023-08-14T13:48:55.000Z","updated":"2023-08-15T02:00:11.116Z","comments":true,"path":"post/24840.html","link":"","permalink":"http://localhost:4000/post/24840.html","excerpt":"","text":"脑PET图像分析和疾病预测挑战赛一、赛事背景脑PET全称为脑部正电子发射计算机断层显像(brain positron emission tomography PET)，是反映脑部病变的基因、分子、代谢及功能状态的显像。它是利用正电子核素标记葡萄糖等人体代谢物作为显像剂，通过病灶对显像剂的摄取来反映其代谢变化，从而为临床提供疾病的生物代谢信息，为脑癫痫病、脑肿瘤、帕金森病、阿尔茨海默综合征等提供了有效的检测手段。可利用脑PET图像检测出轻度认知障碍病灶，并提前介入治疗，从而延缓发病，对后续患者康复治疗有着积极的意义。因此本赛题以轻度认知障碍为例对脑PET图像进行分析与疾病预测。 二、赛事任务为研究基于脑PET图像的疾病预测，本次大赛提供了海量脑PET数据集作为脑PET图像检测数据库的训练样本，参赛者需根据提供的样本构建模型，对轻度认知障碍进行分析和预测。 脑PET图像检测数据库，记录了老年人受试志愿者的脑PET影像资料，其中包括确诊为轻度认知障碍（MCI）患者的脑部影像数据和健康人（NC）的脑部影像数据。 被试者按医学诊断分为两类： NC：健康 MCI：轻度认知障碍 三、 评审规则1.数据说明本次大赛所用脑PET图像检测数据库，图像格式为nii。 表1.大赛数据库 2.评估指标本次竞赛的评价标准采用F1_score，分数越高，效果越好。 问题在基准测试中遇到 以上错误该错误产生的原因是loss接接收到的标签文件应该是 longtensor 类型的数据，但在示例代码中 12345if self.transform is not None: img = self.transform(image = img)[&#x27;image&#x27;]img = img.transpose([2,0,1])return img,torch.from_numpy(np.array(int(&#x27;NC&#x27; in self.img_path[index]))) 返回的标签为tensor类型。要将其改为 1return img, torch.tensor(int(&#x27;NC&#x27; in self.img_path[index]), dtype=torch.long) 将返回的标签转变为longtensor的类型","categories":[],"tags":[]},{"title":"cv","slug":"index","date":"2023-08-07T14:45:47.000Z","updated":"2023-08-11T04:39:58.701Z","comments":true,"path":"post/5801.html","link":"","permalink":"http://localhost:4000/post/5801.html","excerpt":"","text":"【Datawhale CV夏令营】 1 Baseline的基本改进 (I)1 赛题重述1.1 赛题背景机器虽然被大量用到农业生产中，但人还是不可或缺的因素。通过农民身份识别，可以真实客观地记录农民的状态，为农场管理和农产品追溯提供真实的客观数据；较之直接存储视频，可以有效地降低存储空间；自动识别也比人工监管，大幅度提高效率，减少人工成本。 2.2 赛事任务农民身份识别需要对农民进行分类，本次大赛提供了中国农业大学实验室视频制成的图像序列。参赛选手先对图像进行预处理，并制作样本，对图像中的农民进行识别。选手需要自行训练模型，并上传自己训练好的模型和权重。 3.3 赛题数据集本次比赛为参赛选手提供了25名农民身份，每个身份包含10段视频制成的图像序列，选手需要对图像序列进行预处理，打标签，并对农民进行身份识别。 1.4 评价指标本模型依据提交的结果文件，采用Macro-F1进行评价，其中Macro-F1定义如下： 2 Baseline2.1 传统图像特征提取方法通过计算图像中的一些统计指标，如白色像素数目、均值、方差等指标手动构建特征，然后使用机器学习模型进行分类。在此不再赘述。 2.2 深度学习方法采用预训练Resnet18作为骨架，输入为图片，输出为分类结果，如下图所示 3 改进Baseline3.1 梯度裁剪对于模型fθ、度量指标L、输入数据x、对应的标签y和学习率η，有梯度更新 在训练过程中，有可能出现梯度过大导致的震荡情况。因此考虑限制梯度的范数为一定范围，这与信赖域算法思路相似： 这样的操作可以通过paddle.nn.ClipGradByNorm或torch.nn.utils.ClipGradNorm实现。 3.2 集成学习集成学习的思路较为简单，若某一个预测器件的准确率大于50%，那么很多个这样的预测器对结果投票得出的结果准确率将会更高。在对Baseline的改进中，我们引入对某模型效能的置信度函数α⋅，并将其简单地定义为模型的验证准确率的平方根。于是当! 持有N个模型时，期望的分类结果由下面的式子给出： 4 测试结果 仅使用梯度裁剪的Baseline在训练集上训练少于5Epochs 可以在测试集上达到 50% 的准确率 使用梯度裁剪和集成学习的Baseline在训练集上训练9个模型，在测试集上可以达到近 60%的准确率","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-08-07T10:14:01.203Z","updated":"2023-08-11T04:14:35.881Z","comments":true,"path":"post/16107.html","link":"","permalink":"http://localhost:4000/post/16107.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"提点","slug":"提点","permalink":"http://localhost:4000/tags/%E6%8F%90%E7%82%B9/"}]}